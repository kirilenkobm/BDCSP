{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to evaluate the error\n",
    "# negative error never happens\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from collections import Counter\n",
    "from datetime import datetime as dt\n",
    "import statistics\n",
    "from numpy import linspace\n",
    "\n",
    "DATASETS_DIR = \"../tests/input_files/\"\n",
    "# different sets for precision and performance tests\n",
    "DATASETS =['test_100_25_15', 'test_100_25_25', 'test_100_25_35', 'test_100_25_50',\n",
    "           'test_100_25_75', 'test_100_50_15', 'test_100_50_25', 'test_100_50_35',\n",
    "           'test_100_50_50', 'test_100_50_75']\n",
    "# DATASETS = os.listdir(DATASETS_DIR)\n",
    "print(DATASETS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run and collect data\n",
    "CMD_TEMPL = \"../CSP {} {} -v 1 -f\"\n",
    "dataset_data = {}\n",
    "t0 = dt.now()\n",
    "\n",
    "def get_answer(ans, filename, f=None):\n",
    "    \"\"\"Get the smallest program answer.\"\"\"\n",
    "    cmd = CMD_TEMPL.format(filename, ans)\n",
    "    if f:\n",
    "        cmd += \" -z\"\n",
    "    csp_out = subprocess.check_output(cmd, shell=True).decode(\"utf-8\").split(\"\\n\")\n",
    "    if \"True\" in csp_out:\n",
    "        # ok, solved\n",
    "        return ans\n",
    "    max_cov = 0\n",
    "    req_cov = 0\n",
    "    for line in csp_out:\n",
    "        line_data = line.split()\n",
    "        if line.startswith(\"Maximal coverage found is\"):\n",
    "            max_cov = int(line_data[-3])\n",
    "            req_cov = int(line_data[-1])\n",
    "            return ans + req_cov - max_cov\n",
    "            \n",
    "\n",
    "datasets_num = len(DATASETS)\n",
    "for num, dataset in enumerate(DATASETS):\n",
    "    t_d0 = dt.now()\n",
    "    dataset_metadata = dataset.split(\"_\")\n",
    "    str_len = int(dataset_metadata[1])\n",
    "    str_num = int(dataset_metadata[2])\n",
    "    answer = int(dataset_metadata[3])\n",
    "    dataset_data[num] = {\"answers\": [],\n",
    "                         \"answers_f\": [],\n",
    "                         # \"answers_b\": [],\n",
    "                         \"data\": {\"str_len\": str_len,\n",
    "                                  \"str_num\": str_num,\n",
    "                                  \"answer\": answer}}\n",
    "    dataset_dir = os.path.join(DATASETS_DIR, dataset)\n",
    "    contents = os.listdir(dataset_dir)\n",
    "    for fnum, fle in enumerate(contents):\n",
    "        f_path = os.path.join(dataset_dir, fle)\n",
    "        k = get_answer(answer, f_path)\n",
    "        k_f = get_answer(answer, f_path, f=True)\n",
    "        dataset_data[num][\"answers\"].append(k)\n",
    "        dataset_data[num][\"answers_f\"].append(k_f)\n",
    "        # k_bf = get_from_bruteforce(f_path, answer)\n",
    "        # dataset_data[num][\"answers_b\"].append(k_bf)\n",
    "    print(\"Dataset {} / {} done in {}\".format(num + 1, datasets_num, dt.now() - t_d0))\n",
    "\n",
    "print(\"Time spent: {}\".format(dt.now() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare answers with and without F flag\n",
    "all_answers = []\n",
    "all_f_answers = []\n",
    "all_b_answers = []\n",
    "\n",
    "fig = plt.figure(figsize=(20, 7))\n",
    "ax_1 = fig.add_subplot(1, 2, 1)\n",
    "\n",
    "for k, v in dataset_data.items():\n",
    "    # all_answers.extend(v[\"answers\"])\n",
    "    # all_f_answers.extend(v[\"answers_f\"])\n",
    "    # all_b_answers.extend(v[\"answers_b\"])\n",
    "    ax_1.scatter(v[\"answers\"], v[\"answers_f\"], s=15)\n",
    "\n",
    "x = linspace(*ax_1.get_xlim())\n",
    "ax_1.plot(x, x, \"--\", color=\"black\", alpha=0.5)\n",
    "ax_1.set_xlabel(\"Answer without correction\")\n",
    "ax_1.set_ylabel(\"Answer with correction\")\n",
    "ax_1.grid()\n",
    "\n",
    "# ax_2 = fig.add_subplot(1, 2, 2)\n",
    "# x = linspace(*ax_2.get_xlim())\n",
    "# ax_2.plot(x, x)\n",
    "# ax_2.set_xlabel(\"Answer without F flag\")\n",
    "# ax_2.set_ylabel(\"Answer with bruteforce\")\n",
    "# ax_2.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the error\n",
    "for k, v in dataset_data.items():\n",
    "    v[\"errors_f\"] = [a - v[\"data\"][\"answer\"] for a in v[\"answers_f\"]]\n",
    "    # v[\"errors_b\"] = [a - v[\"data\"][\"answer\"] for a in v[\"answers_b\"]]\n",
    "    v[\"errors\"] = [a - v[\"data\"][\"answer\"] for a in v[\"answers\"]]\n",
    "\n",
    "ERR_LIM = 50  # ignore errors > 50% for better plotting\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "to_plot = []\n",
    "titles = []\n",
    "\n",
    "perc_errors = []\n",
    "perc_errors_f = []\n",
    "perc_errors_b = []\n",
    "answers = []\n",
    "\n",
    "for num, (k, v) in enumerate(dataset_data.items(), 1):\n",
    "    # ax = fig.add_subplot(rows, cols, num)\n",
    "    ans = v[\"data\"][\"answer\"]\n",
    "    ans_arr = [ans for _ in range(len(v[\"errors\"]))]\n",
    "    answers.extend(ans_arr)\n",
    "\n",
    "    perc_error = [e / ans * 100 for e in v[\"errors\"]]\n",
    "    perc_error_f = [e / ans * 100 for e in v[\"errors_f\"]]\n",
    "    # perc_error_b = [e / ans * 100 for e in v[\"errors_b\"]]\n",
    "    \n",
    "    perc_error = [e if e < ERR_LIM else ERR_LIM for e in perc_error]\n",
    "    perc_error_f = [e if e < ERR_LIM else ERR_LIM for e in perc_error_f]\n",
    "    # perc_error_b = [e if e < ERR_LIM else ERR_LIM for e in perc_error_b]\n",
    "\n",
    "    perc_errors.extend(perc_error)\n",
    "    perc_errors_f.extend(perc_error_f)\n",
    "    # perc_errors_b.extend(perc_error_b)\n",
    "\n",
    "    to_plot.append(perc_error)\n",
    "    to_plot.append(perc_error_f)\n",
    "    # to_plot.append(perc_error_b)\n",
    "\n",
    "#     to_plot.append(v[\"errors\"])\n",
    "#     to_plot.append(v[\"errors_f\"])\n",
    "    # to_plot.append(v[\"errors_b\"])\n",
    "\n",
    "    title = \"{} {} {}\".format(v[\"data\"][\"str_len\"], v[\"data\"][\"str_num\"], ans)\n",
    "    titles.append(title)\n",
    "    titles.append(title + \"corr\")\n",
    "    # titles.append(title + \"B\")\n",
    "    # ax.set_title(title)\n",
    "ax_1 = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "pos = list(range(1, len(titles) + 1))\n",
    "vp = ax_1.violinplot(to_plot, pos,\n",
    "                     showmeans=True,\n",
    "                     showextrema=True,\n",
    "                     showmedians=True)\n",
    "\n",
    "ax_1.set_title(\"Errors with and without a correction\")\n",
    "ax_1.set_ylabel(\"Error %\")\n",
    "\n",
    "ax_1.set_xticks(pos)\n",
    "ax_1.set_xticklabels(titles, rotation=45, fontsize=8)\n",
    "ax_1.grid(axis=\"y\")\n",
    "\n",
    "colors = [\"navy\", \"navy\",\n",
    "          \"purple\", \"purple\",\n",
    "          \"orange\", \"orange\",\n",
    "          \"green\", \"green\",\n",
    "          \"pink\", \"pink\",\n",
    "          ] * 2\n",
    "vp[\"cmeans\"].set_edgecolor(\"blue\")\n",
    "vp[\"cmedians\"].set_edgecolor(\"green\")\n",
    "vp[\"cbars\"].set_edgecolor(\"grey\")\n",
    "vp[\"cmaxes\"].set_edgecolor(\"grey\")\n",
    "vp[\"cmins\"].set_edgecolor(\"grey\")\n",
    "\n",
    "for vb, color in zip(vp['bodies'], colors):\n",
    "    vb.set_facecolor(color)\n",
    "    vb.set_edgecolor(\"grey\")\n",
    "plt.show()\n",
    "\n",
    "print(vp.keys())\n",
    "\n",
    "print(\"Mean uncorr error: \", statistics.mean(perc_errors))\n",
    "print(\"Median uncorr error: \", statistics.median(perc_errors))\n",
    "\n",
    "print(\"Mean corr error: \", statistics.mean(perc_errors_f))\n",
    "print(\"Median corr error: \", statistics.median(perc_errors_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_err = perc_errors_f.count(0)\n",
    "print(f\"No errors: {no_err} measurements\")\n",
    "print(f\"Errors in: {len(perc_errors_f) - no_err} measurements\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
